import copy
import json
import shutil
import os
import argparse

import numpy as np
import torch
from tqdm.auto import tqdm
from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer

from utils import (
    batch_it,
    clean_data,
    lang_to_m2m_lang_id,
    load_data_from_folder,
    training_step,
    validation_step,
)

# Map user-facing names to internal names for lang_to_m2m_lang_id
LANG_NAME_MAPPING = {
    "egy": "ea",
    "ea": "ea",
    "tnt": "tnt",
    "en": "en",
    "de": "de",
    "lkey": "lKey",
    "worldClass": "wordClass"
}


def extract_data_minimal(data, src_field, tgt_field, src_type, tgt_type):
    """
    Extract data without requiring metadata fields.
    Works with data generated by txt2json.py where metadata may be empty.
    """
    data = filter(
        lambda datapoint: (
            datapoint.get(src_field, "") != ""
            and datapoint.get(tgt_field, "") != ""
        ),
        data,
    )

    data = map(
        lambda datapoint: {
            "source": datapoint[src_field],
            "target": datapoint[tgt_field],
            "metadata": datapoint.get("metadata", {}),
        },
        data,
    )

    data = list(data)
    print(f"{src_type} -> {tgt_type}: Extracted {len(data)} datapoints.")
    return data


def processed_data_minimal(data, lang_pairs):
    """
    Process data for specified language pairs without requiring metadata.
    lang_pairs: list of tuples like [("egy", "tnt"), ("tnt", "en")]
    """
    FIELD_MAPPING = {
        "egy": "source",
        "ea": "source",  # "ea" is the internal name, "egy" is user-facing
        "tnt": "transliteration",
        "en": "target",
        "de": "target",
        "lkey": "lKey",
        "worldClass": "wordClass"
    }
    
    
    result = {}
    for src_type, tgt_type in lang_pairs:
        src_field = FIELD_MAPPING.get(src_type)
        tgt_field = FIELD_MAPPING.get(tgt_type)
        
        if not src_field or not tgt_field:
            print(f"Warning: Unknown type pair {src_type} -> {tgt_type}, skipping")
            continue
        
        if src_type not in result:
            result[src_type] = {}
        
        result[src_type][tgt_type] = extract_data_minimal(
            data, src_field, tgt_field, src_type, tgt_type
        )
    
    return result


# Parse arguments
parser = argparse.ArgumentParser(description='Train hiero-transformer model')
parser.add_argument('--checkpoint', type=str, default=None,
                    help='Path to checkpoint directory to continue training from')
parser.add_argument('--epochs', type=int, default=20,
                    help='Number of epochs to train')
parser.add_argument('--batch_size', type=int, default=16,
                    help='Batch size')
parser.add_argument('--eval_period', type=int, default=1000,
                    help='Evaluation period in steps')
args = parser.parse_args()

# Epochs, batch, periods variables
epochs = args.epochs
batch_size = args.batch_size
eval_period = args.eval_period
total_steps = 0
best_eval_loss = float("inf")
max_models = 1
topk_models = []

# Choose the pairs of languages to train and validate
# Modify these based on your data types from txt2json.py
langs = [
    ("egy", "tnt"),  # Example: if you converted egy->tnt
    # Add more pairs as needed, e.g.:
    # ("tnt", "en"),
    # ("tnt", "de"),
]


# Load data
training_data = load_data_from_folder("training_data")
validation_data = load_data_from_folder("validation_data") if os.path.exists("validation_data") else []

# Clean data
training_data = clean_data(training_data)
if validation_data:
    validation_data = clean_data(validation_data)

# Filter and extract data
# This version doesn't require metadata fields
training_data = processed_data_minimal(training_data, langs)
if validation_data:
    validation_data = processed_data_minimal(validation_data, langs)
else:
    validation_data = {}


# Optional: Adding translations (skip if translations_de2en.json doesn't exist)
if os.path.exists("translations_de2en.json"):
    with open("translations_de2en.json", encoding="utf-8") as f:
        translations = json.load(f)

    for lang in ("ea", "tnt"):
        if lang not in training_data:
            continue
        
        if "de" in training_data[lang] and "en" not in training_data[lang]:
            training_data[lang]["en"] = []
        
        if "de" in training_data[lang] and "en" in training_data[lang]:
            ids_sentence = {
                element["metadata"].get("id_sentence", "")
                for element in training_data[lang]["en"]
                if element["metadata"].get("id_sentence")
            }

            for element in training_data[lang]["de"]:
                id_sentence = element["metadata"].get("id_sentence", "")
                if id_sentence and id_sentence not in ids_sentence:
                    target_text = element["target"]
                    if target_text in translations:
                        new_element = copy.deepcopy(element)
                        new_element["target"] = translations[target_text]
                        new_element["metadata"]["target_lang"] = "en"
                        training_data[lang]["en"].append(new_element)

            print(
                f'{lang} -> en: After translation we have {len(training_data[lang]["en"])} datapoints.'
            )


# Store protected checkpoint path (absolute path for comparison)
protected_checkpoint = None
if args.checkpoint:
    protected_checkpoint = os.path.abspath(args.checkpoint)
    print(f"Loading model from checkpoint: {args.checkpoint}")
    print(f"Protected checkpoint (will not be deleted): {protected_checkpoint}")
    model = M2M100ForConditionalGeneration.from_pretrained(args.checkpoint).to("cuda:0")
    tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_418M")
    
    # Try to load training state if available
    training_state_path = os.path.join(args.checkpoint, "training_state.json")
    if os.path.exists(training_state_path):
        with open(training_state_path, 'r') as f:
            training_state = json.load(f)
            total_steps = training_state.get("total_steps", 0)
            best_eval_loss = training_state.get("best_eval_loss", float("inf"))
            print(f"Resuming from step {total_steps}, best loss: {best_eval_loss}")
    
    # Try to load optimizer state if available
    optimizer_state_path = os.path.join(args.checkpoint, "optimizer.pt")
    optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)
    if os.path.exists(optimizer_state_path):
        try:
            optimizer.load_state_dict(torch.load(optimizer_state_path))
            print("Loaded optimizer state from checkpoint")
        except Exception as e:
            print(f"Could not load optimizer state: {e}, starting with fresh optimizer")
else:
    print("Starting training from base model")
    model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M").to("cuda:0")
    tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_418M")
    optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)


# Training
validation_losses = {}
validation_data_batched = [
    (src_lang, trg_lang, batch)
    for src_lang, values in validation_data.items()
    for trg_lang, data in values.items()
    for batch in batch_it(data, batch_size)
    if (src_lang, trg_lang) in langs
]

for epoch in range(epochs):
    print(f"Starting epoch {epoch + 1}")

    for src_lang, values in training_data.items():
        for data in values.values():
            np.random.shuffle(data)

    training_data_batched = [
        (src_lang, trg_lang, batch)
        for src_lang, values in training_data.items()
        for trg_lang, data in values.items()
        for batch in batch_it(data, batch_size)
        if (src_lang, trg_lang) in langs
    ]

    if not training_data_batched:
        print("Warning: No training data batches found. Check your data and lang pairs.")
        break

    np.random.shuffle(training_data_batched)

    iterator = tqdm(training_data_batched)
    for src_lang, tgt_lang, batch in iterator:
        src_lang_internal = LANG_NAME_MAPPING.get(src_lang, src_lang)
        tgt_lang_internal = LANG_NAME_MAPPING.get(tgt_lang, tgt_lang)
        loss = training_step(
            batch,
            model,
            tokenizer,
            optimizer,
            lang_to_m2m_lang_id.get(src_lang_internal, src_lang_internal),
            lang_to_m2m_lang_id.get(tgt_lang_internal, tgt_lang_internal),
        )
        total_steps += 1
        iterator.set_postfix(
            total_steps=total_steps, loss=loss, src_lang=src_lang, tgt_lang=tgt_lang
        )

        if total_steps % eval_period == 0 and total_steps != 0:
            if validation_data_batched:
                total_eval_loss = 0
                total_eval_tokens = 0

                for src_lang, tgt_lang, batch in validation_data_batched:
                    src_lang_internal = LANG_NAME_MAPPING.get(src_lang, src_lang)
                    tgt_lang_internal = LANG_NAME_MAPPING.get(tgt_lang, tgt_lang)
                    loss, tokens = validation_step(
                        batch,
                        model,
                        tokenizer,
                        lang_to_m2m_lang_id.get(src_lang_internal, src_lang_internal),
                        lang_to_m2m_lang_id.get(tgt_lang_internal, tgt_lang_internal),
                    )
                    total_eval_loss += loss * tokens
                    total_eval_tokens += tokens

                validation_losses[total_steps] = total_eval_loss
                with open("validation_losses.json", "w") as f:
                    json.dump(validation_losses, f)

                if total_eval_loss < best_eval_loss:
                    print(
                        f"The model improved! Old loss={best_eval_loss}, new loss={total_eval_loss}"
                    )
                    fname = f"checkpoint_total_steps={total_steps}_loss={total_eval_loss / total_eval_tokens:.2f}"
                    model.save_pretrained(fname)
                    
                    # Save training state
                    training_state = {
                        "total_steps": total_steps,
                        "best_eval_loss": total_eval_loss,
                        "epoch": epoch + 1
                    }
                    with open(os.path.join(fname, "training_state.json"), "w") as f:
                        json.dump(training_state, f)
                    
                    # Save optimizer state
                    torch.save(optimizer.state_dict(), os.path.join(fname, "optimizer.pt"))
                    
                    topk_models.append(fname)
                    best_eval_loss = total_eval_loss

                    if len(topk_models) > max_models:
                        # Remove oldest checkpoint, but skip if it's the protected checkpoint
                        while topk_models:
                            fname = topk_models.pop(0)
                            fname_abs = os.path.abspath(fname)
                            if protected_checkpoint and fname_abs == protected_checkpoint:
                                print(f"Skipping deletion of protected checkpoint: {fname}")
                                continue
                            shutil.rmtree(fname)
                            print(f"Removing {fname}")
                            break

# Last check before the end
if validation_data_batched:
    total_eval_loss = 0
    total_eval_tokens = 0

    for src_lang, tgt_lang, batch in validation_data_batched:
        src_lang_internal = LANG_NAME_MAPPING.get(src_lang, src_lang)
        tgt_lang_internal = LANG_NAME_MAPPING.get(tgt_lang, tgt_lang)
        loss, tokens = validation_step(
            batch,
            model,
            tokenizer,
            lang_to_m2m_lang_id.get(src_lang_internal, src_lang_internal),
            lang_to_m2m_lang_id.get(tgt_lang_internal, tgt_lang_internal),
        )
        total_eval_loss += loss * tokens
        total_eval_tokens += tokens

    validation_losses[total_steps] = total_eval_loss
    with open("validation_losses.json", "w") as f:
        json.dump(validation_losses, f)

    if total_eval_loss < best_eval_loss:
        print(
            f"The model improved! Old loss={best_eval_loss}, new loss={total_eval_loss}"
        )
        fname = f"checkpoint_total_steps={total_steps}_loss={total_eval_loss / total_eval_tokens:.2f}"
        model.save_pretrained(fname)
        
        # Save training state
        training_state = {
            "total_steps": total_steps,
            "best_eval_loss": total_eval_loss,
            "epoch": epochs
        }
        with open(os.path.join(fname, "training_state.json"), "w") as f:
            json.dump(training_state, f)
        
        # Save optimizer state
        torch.save(optimizer.state_dict(), os.path.join(fname, "optimizer.pt"))
        
        topk_models.append(fname)
        best_eval_loss = total_eval_loss

        if len(topk_models) > max_models:
            # Remove oldest checkpoint, but skip if it's the protected checkpoint
            while topk_models:
                fname = topk_models.pop(0)
                fname_abs = os.path.abspath(fname)
                if protected_checkpoint and fname_abs == protected_checkpoint:
                    print(f"Skipping deletion of protected checkpoint: {fname}")
                    continue
                shutil.rmtree(fname)
                print(f"Removing {fname}")
                break

print("Training completed!")

