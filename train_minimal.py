import copy
import json
import shutil
import os

import numpy as np
import torch
from tqdm.auto import tqdm
from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer

from utils import (
    batch_it,
    clean_data,
    lang_to_m2m_lang_id,
    load_data_from_folder,
    training_step,
    validation_step,
)

# Map user-facing names to internal names for lang_to_m2m_lang_id
LANG_NAME_MAPPING = {
    "egy": "ea",
    "ea": "ea",
    "tnt": "tnt",
    "en": "en",
    "de": "de",
    "lkey": "lKey",
    "worldClass": "wordClass"
}


def extract_data_minimal(data, src_field, tgt_field, src_type, tgt_type):
    """
    Extract data without requiring metadata fields.
    Works with data generated by txt2json.py where metadata may be empty.
    """
    data = filter(
        lambda datapoint: (
            datapoint.get(src_field, "") != ""
            and datapoint.get(tgt_field, "") != ""
        ),
        data,
    )

    data = map(
        lambda datapoint: {
            "source": datapoint[src_field],
            "target": datapoint[tgt_field],
            "metadata": datapoint.get("metadata", {}),
        },
        data,
    )

    data = list(data)
    print(f"{src_type} -> {tgt_type}: Extracted {len(data)} datapoints.")
    return data


def processed_data_minimal(data, lang_pairs):
    """
    Process data for specified language pairs without requiring metadata.
    lang_pairs: list of tuples like [("egy", "tnt"), ("tnt", "en")]
    """
    FIELD_MAPPING = {
        "egy": "source",
        "ea": "source",  # "ea" is the internal name, "egy" is user-facing
        "tnt": "transliteration",
        "en": "target",
        "de": "target",
        "lkey": "lKey",
        "worldClass": "wordClass"
    }
    
    
    result = {}
    for src_type, tgt_type in lang_pairs:
        src_field = FIELD_MAPPING.get(src_type)
        tgt_field = FIELD_MAPPING.get(tgt_type)
        
        if not src_field or not tgt_field:
            print(f"Warning: Unknown type pair {src_type} -> {tgt_type}, skipping")
            continue
        
        if src_type not in result:
            result[src_type] = {}
        
        result[src_type][tgt_type] = extract_data_minimal(
            data, src_field, tgt_field, src_type, tgt_type
        )
    
    return result


# Epochs, batch, periods variables
epochs = 20
batch_size = 16
eval_period = 1000
total_steps = 0
best_eval_loss = float("inf")
max_models = 1
topk_models = []

# Choose the pairs of languages to train and validate
# Modify these based on your data types from txt2json.py
langs = [
    ("egy", "tnt"),  # Example: if you converted egy->tnt
    # Add more pairs as needed, e.g.:
    # ("tnt", "en"),
    # ("tnt", "de"),
]


# Load data
training_data = load_data_from_folder("training_data")
validation_data = load_data_from_folder("validation_data") if os.path.exists("validation_data") else []

# Clean data
training_data = clean_data(training_data)
if validation_data:
    validation_data = clean_data(validation_data)

# Filter and extract data
# This version doesn't require metadata fields
training_data = processed_data_minimal(training_data, langs)
if validation_data:
    validation_data = processed_data_minimal(validation_data, langs)
else:
    validation_data = {}


# Optional: Adding translations (skip if translations_de2en.json doesn't exist)
if os.path.exists("translations_de2en.json"):
    with open("translations_de2en.json", encoding="utf-8") as f:
        translations = json.load(f)

    for lang in ("ea", "tnt"):
        if lang not in training_data:
            continue
        
        if "de" in training_data[lang] and "en" not in training_data[lang]:
            training_data[lang]["en"] = []
        
        if "de" in training_data[lang] and "en" in training_data[lang]:
            ids_sentence = {
                element["metadata"].get("id_sentence", "")
                for element in training_data[lang]["en"]
                if element["metadata"].get("id_sentence")
            }

            for element in training_data[lang]["de"]:
                id_sentence = element["metadata"].get("id_sentence", "")
                if id_sentence and id_sentence not in ids_sentence:
                    target_text = element["target"]
                    if target_text in translations:
                        new_element = copy.deepcopy(element)
                        new_element["target"] = translations[target_text]
                        new_element["metadata"]["target_lang"] = "en"
                        training_data[lang]["en"].append(new_element)

            print(
                f'{lang} -> en: After translation we have {len(training_data[lang]["en"])} datapoints.'
            )


# loading model
model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M").to(
    "cuda:0"
)
tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_418M")
optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)


# Training
validation_losses = {}
validation_data_batched = [
    (src_lang, trg_lang, batch)
    for src_lang, values in validation_data.items()
    for trg_lang, data in values.items()
    for batch in batch_it(data, batch_size)
    if (src_lang, trg_lang) in langs
]

for epoch in range(epochs):
    print(f"Starting epoch {epoch + 1}")

    for src_lang, values in training_data.items():
        for data in values.values():
            np.random.shuffle(data)

    training_data_batched = [
        (src_lang, trg_lang, batch)
        for src_lang, values in training_data.items()
        for trg_lang, data in values.items()
        for batch in batch_it(data, batch_size)
        if (src_lang, trg_lang) in langs
    ]

    if not training_data_batched:
        print("Warning: No training data batches found. Check your data and lang pairs.")
        break

    np.random.shuffle(training_data_batched)

    iterator = tqdm(training_data_batched)
    for src_lang, tgt_lang, batch in iterator:
        src_lang_internal = LANG_NAME_MAPPING.get(src_lang, src_lang)
        tgt_lang_internal = LANG_NAME_MAPPING.get(tgt_lang, tgt_lang)
        loss = training_step(
            batch,
            model,
            tokenizer,
            optimizer,
            lang_to_m2m_lang_id.get(src_lang_internal, src_lang_internal),
            lang_to_m2m_lang_id.get(tgt_lang_internal, tgt_lang_internal),
        )
        total_steps += 1
        iterator.set_postfix(
            total_steps=total_steps, loss=loss, src_lang=src_lang, tgt_lang=tgt_lang
        )

        if total_steps % eval_period == 0 and total_steps != 0:
            if validation_data_batched:
                total_eval_loss = 0
                total_eval_tokens = 0

                for src_lang, tgt_lang, batch in validation_data_batched:
                    src_lang_internal = LANG_NAME_MAPPING.get(src_lang, src_lang)
                    tgt_lang_internal = LANG_NAME_MAPPING.get(tgt_lang, tgt_lang)
                    loss, tokens = validation_step(
                        batch,
                        model,
                        tokenizer,
                        lang_to_m2m_lang_id.get(src_lang_internal, src_lang_internal),
                        lang_to_m2m_lang_id.get(tgt_lang_internal, tgt_lang_internal),
                    )
                    total_eval_loss += loss * tokens
                    total_eval_tokens += tokens

                validation_losses[total_steps] = total_eval_loss
                with open("validation_losses.json", "w") as f:
                    json.dump(validation_losses, f)

                if total_eval_loss < best_eval_loss:
                    print(
                        f"The model improved! Old loss={best_eval_loss}, new loss={total_eval_loss}"
                    )
                    fname = f"checkpoint_total_steps={total_steps}_loss={total_eval_loss / total_eval_tokens:.2f}"
                    model.save_pretrained(fname)
                    topk_models.append(fname)
                    best_eval_loss = total_eval_loss

                    if len(topk_models) > max_models:
                        fname = topk_models.pop(0)
                        shutil.rmtree(fname)
                        print(f"Removing {fname}")

# Last check before the end
if validation_data_batched:
    total_eval_loss = 0
    total_eval_tokens = 0

    for src_lang, tgt_lang, batch in validation_data_batched:
        src_lang_internal = LANG_NAME_MAPPING.get(src_lang, src_lang)
        tgt_lang_internal = LANG_NAME_MAPPING.get(tgt_lang, tgt_lang)
        loss, tokens = validation_step(
            batch,
            model,
            tokenizer,
            lang_to_m2m_lang_id.get(src_lang_internal, src_lang_internal),
            lang_to_m2m_lang_id.get(tgt_lang_internal, tgt_lang_internal),
        )
        total_eval_loss += loss * tokens
        total_eval_tokens += tokens

    validation_losses[total_steps] = total_eval_loss
    with open("validation_losses.json", "w") as f:
        json.dump(validation_losses, f)

    if total_eval_loss < best_eval_loss:
        print(
            f"The model improved! Old loss={best_eval_loss}, new loss={total_eval_loss}"
        )
        fname = f"checkpoint_total_steps={total_steps}_loss={total_eval_loss / total_eval_tokens:.2f}"
        model.save_pretrained(fname)
        topk_models.append(fname)
        best_eval_loss = total_eval_loss

        if len(topk_models) > max_models:
            fname = topk_models.pop(0)
            shutil.rmtree(fname)
            print(f"Removing {fname}")

print("Training completed!")

